{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7116f2b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.6' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/DELL/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Custom modules\n",
    "from config import Config\n",
    "from src.data.data_loader import BrainTumorDataLoader\n",
    "from utils.visualization import DataVisualization\n",
    "from utils.helpers import set_random_seeds, setup_logging, validate_dataset_structure\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_random_seeds(Config.RANDOM_SEED)\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging()\n",
    "\n",
    "print(\"üß† Brain Tumor Detection - Phase 1: Data Exploration\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e17cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display system information\n",
    "from utils.helpers import get_system_info\n",
    "system_info = get_system_info()\n",
    "\n",
    "print(\"üìä System Information:\")\n",
    "for key, value in system_info.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nüìÅ Dataset Validation:\")\n",
    "dataset_valid = validate_dataset_structure(Config.DATASET_PATH)\n",
    "\n",
    "if not dataset_valid:\n",
    "    raise Exception(\"Dataset structure validation failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acacae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = BrainTumorDataLoader()\n",
    "\n",
    "# Get comprehensive dataset overview\n",
    "print(\"üìà Dataset Overview Analysis...\")\n",
    "overview = data_loader.get_dataset_overview()\n",
    "\n",
    "print(\"\\nüéØ DATASET STATISTICS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Training set statistics\n",
    "print(\"\\nüìö TRAINING SET:\")\n",
    "total_train = 0\n",
    "for class_name in Config.CLASS_NAMES:\n",
    "    count = overview['training'][class_name]['count']\n",
    "    total_train += count\n",
    "    print(f\"   {class_name:12}: {count:4d} images\")\n",
    "\n",
    "print(f\"   {'TOTAL':12}: {total_train:4d} images\")\n",
    "\n",
    "# Testing set statistics  \n",
    "print(\"\\nüß™ TESTING SET:\")\n",
    "total_test = 0\n",
    "for class_name in Config.CLASS_NAMES:\n",
    "    count = overview['testing'][class_name]['count']\n",
    "    total_test += count\n",
    "    print(f\"   {class_name:12}: {count:4d} images\")\n",
    "\n",
    "print(f\"   {'TOTAL':12}: {total_test:4d} images\")\n",
    "\n",
    "print(f\"\\nüìä OVERALL DATASET SIZE: {total_train + total_test} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualization\n",
    "viz = DataVisualization()\n",
    "\n",
    "print(\"üìä Class Distribution Analysis...\")\n",
    "\n",
    "# Create class distribution visualization\n",
    "viz.plot_class_distribution(overview, save_path=Config.FIGURES_PATH / \"class_distribution.png\")\n",
    "\n",
    "# Calculate class balance metrics\n",
    "train_counts = [overview['training'][class_name]['count'] for class_name in Config.CLASS_NAMES]\n",
    "test_counts = [overview['testing'][class_name]['count'] for class_name in Config.CLASS_NAMES]\n",
    "\n",
    "# Class imbalance analysis\n",
    "train_imbalance_ratio = max(train_counts) / min(train_counts)\n",
    "test_imbalance_ratio = max(test_counts) / min(test_counts)\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  CLASS BALANCE ANALYSIS:\")\n",
    "print(f\"   Training Imbalance Ratio: {train_imbalance_ratio:.2f}\")\n",
    "print(f\"   Testing Imbalance Ratio:  {test_imbalance_ratio:.2f}\")\n",
    "\n",
    "if train_imbalance_ratio > 2.0:\n",
    "    print(\"   ‚ö†Ô∏è  WARNING: Significant class imbalance detected!\")\n",
    "    print(\"   üí° Consider: Class weighting, SMOTE, or stratified sampling\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Class distribution is reasonably balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e96588",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüñºÔ∏è  Loading Sample Images...\")\n",
    "\n",
    "# Load sample images for visualization\n",
    "sample_images = data_loader.load_sample_images(n_samples=5)\n",
    "\n",
    "# Display sample images\n",
    "viz.plot_sample_images(sample_images, save_path=Config.FIGURES_PATH / \"sample_images.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2078b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Analyzing Image Properties...\")\n",
    "\n",
    "# Analyze training set properties\n",
    "print(\"   Analyzing training set...\")\n",
    "train_analysis = data_loader.analyze_image_properties(split='training')\n",
    "\n",
    "# Analyze testing set properties\n",
    "print(\"   Analyzing testing set...\")\n",
    "test_analysis = data_loader.analyze_image_properties(split='testing')\n",
    "\n",
    "# Combine datasets for comprehensive analysis\n",
    "train_analysis['split'] = 'training'\n",
    "test_analysis['split'] = 'testing'\n",
    "combined_analysis = pd.concat([train_analysis, test_analysis], ignore_index=True)\n",
    "\n",
    "print(f\"\\nüìã IMAGE PROPERTIES SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nüìê DIMENSIONS:\")\n",
    "print(f\"   Width  - Mean: {combined_analysis['width'].mean():.1f}, Std: {combined_analysis['width'].std():.1f}\")\n",
    "print(f\"   Height - Mean: {combined_analysis['height'].mean():.1f}, Std: {combined_analysis['height'].std():.1f}\")\n",
    "\n",
    "print(\"\\nüíæ FILE SIZES:\")\n",
    "print(f\"   Mean: {combined_analysis['file_size_kb'].mean():.1f} KB\")\n",
    "print(f\"   Min:  {combined_analysis['file_size_kb'].min():.1f} KB\")\n",
    "print(f\"   Max:  {combined_analysis['file_size_kb'].max():.1f} KB\")\n",
    "\n",
    "print(\"\\nüé® INTENSITY STATISTICS:\")\n",
    "print(f\"   Mean Intensity - Mean: {combined_analysis['mean_intensity'].mean():.1f}\")\n",
    "print(f\"   Std Intensity  - Mean: {combined_analysis['std_intensity'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis by class\n",
    "print(\"\\nüìä DETAILED ANALYSIS BY CLASS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class_stats = combined_analysis.groupby('class').agg({\n",
    "    'width': ['mean', 'std', 'min', 'max'],\n",
    "    'height': ['mean', 'std', 'min', 'max'], \n",
    "    'file_size_kb': ['mean', 'std', 'min', 'max'],\n",
    "    'mean_intensity': ['mean', 'std', 'min', 'max'],\n",
    "    'std_intensity': ['mean', 'std', 'min', 'max']\n",
    "}).round(2)\n",
    "\n",
    "display(class_stats)\n",
    "\n",
    "# Check for dimension consistency\n",
    "unique_dimensions = combined_analysis[['width', 'height']].drop_duplicates()\n",
    "print(f\"\\nüìè UNIQUE IMAGE DIMENSIONS: {len(unique_dimensions)}\")\n",
    "\n",
    "if len(unique_dimensions) > 1:\n",
    "    print(\"‚ö†Ô∏è  Multiple image dimensions detected:\")\n",
    "    print(unique_dimensions.value_counts())\n",
    "    print(\"üí° Recommendation: Standardize all images to single dimension\")\n",
    "else:\n",
    "    print(\"‚úÖ All images have consistent dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093622a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìà Creating Comprehensive Visualizations...\")\n",
    "\n",
    "# Create detailed property analysis plots\n",
    "viz.plot_image_properties_analysis(combined_analysis, \n",
    "                                 save_path=Config.FIGURES_PATH / \"image_properties_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53579f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç DATA QUALITY ASSESSMENT:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check for potential issues\n",
    "quality_issues = []\n",
    "\n",
    "# 1. Check for extremely small or large files\n",
    "size_threshold_low = combined_analysis['file_size_kb'].quantile(0.05)\n",
    "size_threshold_high = combined_analysis['file_size_kb'].quantile(0.95)\n",
    "\n",
    "small_files = combined_analysis[combined_analysis['file_size_kb'] < size_threshold_low]\n",
    "large_files = combined_analysis[combined_analysis['file_size_kb'] > size_threshold_high]\n",
    "\n",
    "if len(small_files) > 0:\n",
    "    quality_issues.append(f\"Found {len(small_files)} unusually small files\")\n",
    "if len(large_files) > 0:\n",
    "    quality_issues.append(f\"Found {len(large_files)} unusually large files\")\n",
    "\n",
    "# 2. Check for extreme aspect ratios\n",
    "extreme_aspect = combined_analysis[\n",
    "    (combined_analysis['aspect_ratio'] < 0.8) | \n",
    "    (combined_analysis['aspect_ratio'] > 1.2)\n",
    "]\n",
    "\n",
    "if len(extreme_aspect) > 0:\n",
    "    quality_issues.append(f\"Found {len(extreme_aspect)} images with extreme aspect ratios\")\n",
    "\n",
    "# 3. Check for very dark or bright images\n",
    "dark_images = combined_analysis[combined_analysis['mean_intensity'] < 20]\n",
    "bright_images = combined_analysis[combined_analysis['mean_intensity'] > 200]\n",
    "\n",
    "if len(dark_images) > 0:\n",
    "    quality_issues.append(f\"Found {len(dark_images)} very dark images\")\n",
    "if len(bright_images) > 0:\n",
    "    quality_issues.append(f\"Found {len(bright_images)} very bright images\")\n",
    "\n",
    "# Display quality assessment results\n",
    "if quality_issues:\n",
    "    print(\"‚ö†Ô∏è  POTENTIAL QUALITY ISSUES DETECTED:\")\n",
    "    for issue in quality_issues:\n",
    "        print(f\"   ‚Ä¢ {issue}\")\n",
    "    print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "    print(\"   ‚Ä¢ Review flagged images manually\")\n",
    "    print(\"   ‚Ä¢ Consider preprocessing to normalize intensity\")\n",
    "    print(\"   ‚Ä¢ Apply consistent resizing/cropping\")\n",
    "else:\n",
    "    print(\"‚úÖ No major quality issues detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ff644",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéõÔ∏è  Creating Interactive Dashboard...\")\n",
    "\n",
    "# Create interactive Plotly dashboard\n",
    "interactive_fig = viz.create_interactive_dashboard(combined_analysis)\n",
    "interactive_fig.show()\n",
    "\n",
    "# Save interactive plot\n",
    "interactive_fig.write_html(str(Config.FIGURES_PATH / \"interactive_dashboard.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f403101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ KEY INSIGHTS & RECOMMENDATIONS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Class distribution insights\n",
    "if train_imbalance_ratio > 2.0:\n",
    "    insights.append(\"‚öñÔ∏è  ADDRESS CLASS IMBALANCE: Use class weights or data augmentation\")\n",
    "\n",
    "# Dimension insights\n",
    "if len(unique_dimensions) > 1:\n",
    "    insights.append(\"üìè STANDARDIZE DIMENSIONS: Resize all images to consistent size\")\n",
    "\n",
    "# File size insights\n",
    "size_variation = combined_analysis['file_size_kb'].std() / combined_analysis['file_size_kb'].mean()\n",
    "if size_variation > 0.5:\n",
    "    insights.append(\"üíæ HIGH FILE SIZE VARIATION: Consider compression standardization\")\n",
    "\n",
    "# Intensity insights\n",
    "intensity_variation = combined_analysis.groupby('class')['mean_intensity'].std().mean()\n",
    "if intensity_variation > 30:\n",
    "    insights.append(\"üé® HIGH INTENSITY VARIATION: Apply histogram equalization\")\n",
    "\n",
    "# General recommendations\n",
    "insights.extend([\n",
    "    \"üîç PREPROCESSING PIPELINE: Implement consistent preprocessing\",\n",
    "    \"üìä DATA AUGMENTATION: Use rotation, flip, zoom to increase diversity\",\n",
    "    \"üß† FEATURE EXTRACTION: Consider K-means segmentation for region analysis\",\n",
    "    \"‚ö° MODEL STRATEGY: Ensemble approach with CNN + traditional ML\"\n",
    "])\n",
    "\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i:2d}. {insight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüíæ Exporting Analysis Results...\")\n",
    "\n",
    "# Save analysis results\n",
    "results_summary = {\n",
    "    'dataset_overview': overview,\n",
    "    'total_images': total_train + total_test,\n",
    "    'class_imbalance_ratio': {\n",
    "        'training': train_imbalance_ratio,\n",
    "        'testing': test_imbalance_ratio\n",
    "    },\n",
    "    'image_dimensions': {\n",
    "        'unique_dimensions': len(unique_dimensions),\n",
    "        'most_common_size': (\n",
    "            int(combined_analysis['width'].mode()[0]), \n",
    "            int(combined_analysis['height'].mode()[0])\n",
    "        )\n",
    "    },\n",
    "    'quality_issues': quality_issues,\n",
    "    'recommendations': insights\n",
    "}\n",
    "\n",
    "# Save to JSON for later use\n",
    "import json\n",
    "with open(Config.REPORTS_PATH / 'phase1_analysis_summary.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "# Save detailed analysis DataFrame\n",
    "combined_analysis.to_csv(Config.REPORTS_PATH / 'detailed_image_analysis.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Analysis results exported successfully!\")\n",
    "print(f\"   üìÅ Summary: {Config.REPORTS_PATH / 'phase1_analysis_summary.json'}\")\n",
    "print(f\"   üìä Detailed data: {Config.REPORTS_PATH / 'detailed_image_analysis.csv'}\")\n",
    "print(f\"   üñºÔ∏è  Visualizations: {Config.FIGURES_PATH}\")\n",
    "\n",
    "print(\"\\nüéâ PHASE 1 ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"üìã Next Steps:\")\n",
    "print(\"   1. Review generated visualizations and reports\")\n",
    "print(\"   2. Plan preprocessing strategy based on insights\")\n",
    "print(\"   3. Move to Phase 2: Preprocessing & Feature Engineering\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
